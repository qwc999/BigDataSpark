from pyspark.sql import SparkSession, Window
import pyspark.sql.functions as F

# --- Application Configuration ---
APP_NAME = "DWHtoClickHouseReports"
POSTGRES_JDBC_JAR_PATH = "/opt/spark/jars/postgresql-42.6.0.jar"
CLICKHOUSE_JDBC_JAR_PATH = "/opt/spark/jars/clickhouse-jdbc-0.4.6.jar"

# --- PostgreSQL Connection Details (–ò—ù—Ç–æ—á–Ω–∏–∫ DWH) ---
PG_DB_URL = "jdbc:postgresql://postgres_db:5432/bigdata"
PG_DB_PROPERTIES = {
    "user": "admin",
    "password": "admin",
    "driver": "org.postgresql.Driver"
}

# --- ClickHouse Connection Details (–¶–µ–ª–µ–≤–∞—ù –ë–î –¥–ª—ù –æ—Ç—á–µ—Ç–æ–≤) ---
CH_DB_URL = "jdbc:clickhouse://clickhouse:8123/default"
CH_DB_PROPERTIES = {
    "driver": "com.clickhouse.jdbc.ClickHouseDriver",
}

CH_WRITE_MODE = "overwrite"
CH_TABLE_ENGINE_OPTIONS = "ENGINE = MergeTree() ORDER BY tuple()"

def initialize_spark_session(app_name, pg_jar, ch_jar):
    """Initializes and returns a SparkSession."""
    print(f"Initializing Spark session: {app_name} with JARs: {pg_jar}, {ch_jar}")
    jars_path = f"{pg_jar},{ch_jar}"
    session = (
        SparkSession.builder
        .appName(app_name)
        .config("spark.jars", jars_path)
        .getOrCreate()
    )
    print("Spark session initialized.")
    return session


def load_dwh_table(spark, table_name):
    """Loads a table from PostgreSQL DWH."""
    print(f"Loading DWH table from PostgreSQL: {table_name}")
    df = (
        spark.read
        .format("jdbc")
        .option("url", PG_DB_URL)
        .option("dbtable", table_name)
        .options(**PG_DB_PROPERTIES)
        .load()
    )
    print(f"Loaded {df.count()} rows from {table_name}.")
    return df


def save_report_to_clickhouse(report_df, report_table_name, order_by_cols=None):
    """Saves a report DataFrame to a ClickHouse table."""
    print(f"Saving report to ClickHouse table: {report_table_name}")

    current_ch_table_engine = CH_TABLE_ENGINE_OPTIONS
    if "MergeTree" in CH_TABLE_ENGINE_OPTIONS and order_by_cols:
        order_by_clause = ", ".join(order_by_cols)
        current_ch_table_engine = f"ENGINE = MergeTree() ORDER BY ({order_by_clause})"
    elif "MergeTree" in CH_TABLE_ENGINE_OPTIONS and not order_by_cols and "ORDER BY tuple()" not in CH_TABLE_ENGINE_OPTIONS:
        current_ch_table_engine = f"{CH_TABLE_ENGINE_OPTIONS.replace('ORDER BY tuple()', '')} ORDER BY tuple()"

    (
        report_df.write
        .format("jdbc")
        .mode(CH_WRITE_MODE)
        .option("url", CH_DB_URL)
        .option("dbtable", report_table_name)
        .option("createTableOptions", current_ch_table_engine)
        .options(**CH_DB_PROPERTIES)
        .save()
    )
    print(f"Successfully saved report {report_table_name} with {report_df.count()} rows.")


def generate_reports(spark):
    """Generates and saves all required reports to ClickHouse."""

    # 1. Load data from DWH (PostgreSQL)
    print("Loading base DWH tables...")
    fact_sales_df = load_dwh_table(spark, "f_sales").cache()
    d_product_df = load_dwh_table(spark, "d_products").cache()
    d_customer_df = load_dwh_table(spark, "d_customers").cache()
    d_date_df = load_dwh_table(spark, "d_date").cache()
    d_store_df = load_dwh_table(spark, "d_stores").cache()
    d_supplier_df = load_dwh_table(spark, "d_suppliers").cache()
    print("Base DWH tables loaded and cached.")

    # --- –í–∏—Ç—Ä–∏–Ω–∞ –ø—Ä–æ–¥–∞–∂ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º ---
    print("Generating Product Sales Mart...")
    # 1.1 –¢–æ–ø-10 —ù–∞–º—ã—Ö –ø—Ä–æ–¥–∞–≤–∞–µ–º—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (–ø–æ –∫–æ–ª–∏—á–µ—ù—Ç–≤—É)
    report_top_10_products = (
        fact_sales_df.groupBy("product_id")
        .agg(
            F.sum("sale_quantity").alias("total_quantity_sold"),
            F.sum("sale_total_price").alias("total_revenue_generated")
        )
        .join(d_product_df, "product_id")
        .select(
            d_product_df.product_id.alias("business_product_id"),
            d_product_df.name.alias("product_name"),
            d_product_df.category.alias("product_category"),
            "total_quantity_sold",
            "total_revenue_generated"
        )
        .orderBy(F.desc("total_quantity_sold"))
        .limit(10)
    )
    save_report_to_clickhouse(report_top_10_products, "mart_top_10_selling_products",
                              order_by_cols=["total_quantity_sold"])

    # 1.2 –û–±—â–∞—ù –≤—ã—Ä—É—á–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—ù–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤
    report_revenue_by_category = (
        fact_sales_df.join(d_product_df, "product_id")
        .groupBy(d_product_df.category.alias("product_category"))
        .agg(F.sum("sale_total_price").alias("category_total_revenue"))
        .orderBy(F.desc("category_total_revenue"))
    )
    save_report_to_clickhouse(report_revenue_by_category, "mart_revenue_by_product_category",
                              order_by_cols=["product_category"])

    # 1.3 –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –∏ –∫–æ–ª–∏—á–µ—ù—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—ù –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    report_product_feedback_summary = d_product_df.select(
        d_product_df.product_id.alias("business_product_id"),
        d_product_df.name.alias("product_name"),
        d_product_df.category.alias("product_category"),
        d_product_df.rating.alias("average_rating"),
        d_product_df.reviews.alias("number_of_reviews")
    ).orderBy(F.desc("number_of_reviews"))
    save_report_to_clickhouse(report_product_feedback_summary, "mart_product_feedback_summary",
                              order_by_cols=["business_product_id"])

    # --- –í–∏—Ç—Ä–∏–Ω–∞ –ø—Ä–æ–¥–∞–∂ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º ---
    print("Generating Customer Sales Mart...")
    # 2.1 –¢–æ–ø-10 –∫–ª–∏–µ–Ω—Ç–æ–≤ —ù –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ–±—â–µ–π —ù—É–º–º–æ–π –ø–æ–∫—É–ø–æ–∫
    report_top_10_customers_by_purchase = (
        fact_sales_df.groupBy("customer_id")
        .agg(F.sum("sale_total_price").alias("customer_total_purchases"))
        .join(d_customer_df, "customer_id")
        .select(
            d_customer_df.customer_id.alias("business_customer_id"),
            d_customer_df.first_name,
            d_customer_df.last_name,
            d_customer_df.country.alias("customer_country"),
            "customer_total_purchases"
        )
        .orderBy(F.desc("customer_total_purchases"))
        .limit(10)
    )
    save_report_to_clickhouse(report_top_10_customers_by_purchase, "mart_top_10_customers_by_purchase",
                              order_by_cols=["customer_total_purchases"])

    # 2.2 –†–∞—ù–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —ù—Ç—Ä–∞–Ω–∞–º
    report_customer_distribution_by_country = (
        d_customer_df.groupBy(d_customer_df.country.alias("customer_country"))
        .agg(F.countDistinct(d_customer_df.customer_id).alias("distinct_customer_count"))
        .orderBy(F.desc("distinct_customer_count"))
    )
    save_report_to_clickhouse(report_customer_distribution_by_country, "mart_customer_distribution_by_country",
                              order_by_cols=["customer_country"])

    # 2.3 –°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –¥–ª—ù –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
    # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—ù—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (–ø—Ä–æ–¥–∞–∂) –Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —ù—Ä–µ–¥–Ω–∏–π —á–µ–∫
    # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –∫–∞–∂–¥–∞—ù —ù—Ç—Ä–æ–∫–∞ –≤ fact_sales - —ù—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—ù —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—ù (—á–µ–∫)
    # –ï—ù–ª–∏ –Ω–µ—ù–∫–æ–ª—å–∫–æ —ù—Ç—Ä–æ–∫ –≤ fact_sales –º–æ–≥—É—Ç –æ—Ç–Ω–æ—ù–∏—Ç—å—ù—ù –∫ –æ–¥–Ω–æ–º—É —á–µ–∫—É, —Ç–æ –Ω—É–∂–Ω–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ ID —á–µ–∫–∞.
    # –í –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ fact_sales –∫–∞–∂–¥–∞—ù —ù—Ç—Ä–æ–∫–∞ - —ù—Ç–æ –∑–∞–ø–∏—ù—å –æ –ø—Ä–æ–¥–∞–∂–µ —Ç–æ–≤–∞—Ä–∞, –≤–æ–∑–º–æ–∂–Ω–æ, –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ —á–µ–∫–∞.
    # –î–ª—ù "—ù—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞" –ª—É—á—à–µ –∏–º–µ—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—á–µ–∫–∞).
    # –ï—ù–ª–∏ –µ–≥–æ –Ω–µ—Ç, —Ç–æ –º–æ–∂–Ω–æ —ù—á–∏—Ç–∞—Ç—å —ù—Ä–µ–¥–Ω—é—é —ù—É–º–º—É *–æ–¥–Ω–æ–π –∑–∞–ø–∏—ù–∏ –æ –ø—Ä–æ–¥–∞–∂–µ* –Ω–∞ –∫–ª–∏–µ–Ω—Ç–∞.
    # –ò–ª–∏, –µ—ù–ª–∏ sale_total_price - —ù—Ç–æ —É–∂–µ —ù—É–º–º–∞ –≤—ù–µ–≥–æ —á–µ–∫–∞ (—á—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—ù—Ç–Ω–æ, –µ—ù–ª–∏ –µ—ù—Ç—å sale_quantity),
    # —Ç–æ –Ω—É–∂–Ω–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–∞—á–µ.
    # –ü—Ä–∏–º–µ—Ä –Ω–∏–∂–µ —ù—á–∏—Ç–∞–µ—Ç —ù—Ä–µ–¥–Ω—é—é —ù—Ç–æ–∏–º–æ—ù—Ç—å –û–î–ù–û–ô –°–¢–†–û–ö–ò –≤ fact_sales –¥–ª—ù –∫–ª–∏–µ–Ω—Ç–∞.
    # –î–ª—ù –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —ù—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞, –µ—ù–ª–∏ –Ω–µ—ù–∫–æ–ª—å–∫–æ —ù—Ç—Ä–æ–∫ –≤ fact_sales —ù—Ç–æ –æ–¥–∏–Ω —á–µ–∫,
    # –Ω–∞–º –Ω—É–∂–µ–Ω –±—ã–ª –±—ã ID —á–µ–∫–∞.
    # (sum(total_price) / count(quantity)) - —ù—Ç–æ —ù–∫–æ—Ä–µ–µ —ù—Ä–µ–¥–Ω—ù—ù —Ü–µ–Ω–∞ *–∑–∞ –µ–¥–∏–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞* –ø–æ –∫–ª–∏–µ–Ω—Ç—É, –µ—ù–ª–∏ quantity > 1.
    # sum(total_price) / count(distinct transaction_id) –µ—ù–ª–∏ –±—ã –æ–Ω –±—ã–ª.
    # –¢–∞–∫ –∫–∞–∫ –µ–≥–æ –Ω–µ—Ç, "sale_quantity" –∫–∞–∫ "–∫–æ–ª–∏—á–µ—ù—Ç–≤–æ —á–µ–∫–æ–≤/—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π".
    # –ù–û! sale_quantity –≤ fact_sales - —ù—Ç–æ –∫–æ–ª–∏—á–µ—ù—Ç–≤–æ —Ç–æ–≤–∞—Ä–∞ –≤ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏.
    # –ï—ù–ª–∏ –º—ã —Ö–æ—Ç–∏–º —ù—Ä–µ–¥–Ω–∏–π —á–µ–∫, —Ç–æ –Ω—É–∂–Ω–æ –∫–æ–ª–∏—á–µ—ù—Ç–≤–æ –£–ù–ò–ö–ù–õ–¨–ù–´–• —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π.
    # –ï—ù–ª–∏ –∫–∞–∂–¥–∞—ù —ù—Ç—Ä–æ–∫–∞ –≤ fact_sales - —ù—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω–∞—ù —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—ù, —Ç–æ count(*) –∏–ª–∏ count("sale_sk")
    report_avg_customer_order_value = (
        fact_sales_df.groupBy("customer_id")
        .agg(
            (F.sum("sale_total_price") / F.countDistinct("sale_sk")).alias("avg_transaction_value_per_customer")
        )
        .join(d_customer_df, "customer_id")
        .select(
            d_customer_df.customer_id.alias("business_customer_id"),
            d_customer_df.first_name,
            d_customer_df.last_name,
            "avg_transaction_value_per_customer"
        )
        .orderBy(F.desc("avg_transaction_value_per_customer"))
    )
    save_report_to_clickhouse(report_avg_customer_order_value, "mart_avg_customer_order_value",
                              order_by_cols=["business_customer_id"])

    # --- –í–∏—Ç—Ä–∏–Ω–∞ –ø—Ä–æ–¥–∞–∂ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ---
    print("Generating Time-based Sales Mart...")
    # 3.1 –ú–µ—ù—ù—á–Ω—ã–µ –∏ –≥–æ–¥–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø—Ä–æ–¥–∞–∂
    sales_with_date_details_df = fact_sales_df.join(d_date_df, "date_id")

    report_monthly_sales_trends = (
        sales_with_date_details_df
        .groupBy(d_date_df.year, d_date_df.month, d_date_df.month_name)
        .agg(
            F.sum("sale_total_price").alias("monthly_total_revenue"),
            F.sum("sale_quantity").alias("monthly_total_quantity_sold")
        )
        .orderBy(d_date_df.year, d_date_df.month)
    )
    save_report_to_clickhouse(report_monthly_sales_trends, "mart_monthly_sales_trends", order_by_cols=["year", "month"])

    report_yearly_sales_trends = (
        sales_with_date_details_df
        .groupBy(d_date_df.year)
        .agg(
            F.sum("sale_total_price").alias("yearly_total_revenue"),
            F.sum("sale_quantity").alias("yearly_total_quantity_sold")
        )
        .orderBy(d_date_df.year)
    )
    save_report_to_clickhouse(report_yearly_sales_trends, "mart_yearly_sales_trends", order_by_cols=["year"])

    # 3.2 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—Ä—É—á–∫–∏ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã (MoM - Month over Month)
    window_spec_mom = Window.orderBy("year", "month")
    report_mom_revenue_comparison = (
        report_monthly_sales_trends
        .withColumn("previous_month_revenue", F.lag("monthly_total_revenue", 1, 0).over(window_spec_mom))
        .withColumn(
            "mom_revenue_change",
            (F.col("monthly_total_revenue") - F.col("previous_month_revenue"))
        )
        .withColumn(
            "mom_revenue_change_percent",
            F.when(F.col("previous_month_revenue") != 0,
                   F.round((F.col("mom_revenue_change") / F.col("previous_month_revenue")) * 100, 2)
                   ).otherwise(0)
        )
        .select(
            "year", "month", "month_name",
            "monthly_total_revenue",
            "previous_month_revenue",
            "mom_revenue_change",
            "mom_revenue_change_percent"
        )
    )
    save_report_to_clickhouse(report_mom_revenue_comparison, "mart_mom_revenue_comparison",
                              order_by_cols=["year", "month"])

    # 3.3 –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∑–∞–∫–∞–∑–∞ –ø–æ –º–µ—ù—ù—Ü–∞–º
    # –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∑–∞–∫–∞–∑–∞ = –û–±—â–∞—ù –≤—ã—Ä—É—á–∫–∞ –∑–∞ –º–µ—ù—ù—Ü / –ö–æ–ª–∏—á–µ—ù—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∑–∞ –º–µ—ù—ù—Ü
    report_avg_order_size_by_month = (
        sales_with_date_details_df
        .groupBy(d_date_df.year, d_date_df.month, d_date_df.month_name)
        .agg(
            (F.sum("sale_total_price") / F.countDistinct("sale_sk")).alias("avg_monthly_order_size")
        )
        .orderBy(d_date_df.year, d_date_df.month)
    )
    save_report_to_clickhouse(report_avg_order_size_by_month, "mart_avg_order_size_by_month",
                              order_by_cols=["year", "month"])

    # --- –í–∏—Ç—Ä–∏–Ω–∞ –ø—Ä–æ–¥–∞–∂ –ø–æ –º–∞–≥–∞–∑–∏–Ω–∞–º ---
    print("Generating Store Sales Mart...")
    sales_with_store_details_df = fact_sales_df.join(d_store_df, "store_id")

    # 4.1 –¢–æ–ø-5 –º–∞–≥–∞–∑–∏–Ω–æ–≤ — –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤—ã—Ä—É—á–∫–æ–π
    report_top_5_stores_by_revenue = (
        sales_with_store_details_df.groupBy(d_store_df.name.alias("store_name"), d_store_df.city,
                                            d_store_df.country)
        .agg(F.sum("sale_total_price").alias("store_total_revenue"))
        .orderBy(F.desc("store_total_revenue"))
        .limit(5)
    )
    save_report_to_clickhouse(report_top_5_stores_by_revenue, "mart_top_5_stores_by_revenue",
                              order_by_cols=["store_total_revenue"])

    # 4.2 –†–∞—ù–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –ø–æ –≥–æ—Ä–æ–¥–∞–º –∏ —ù—Ç—Ä–∞–Ω–∞–º (–º–∞–≥–∞–∑–∏–Ω–æ–≤)
    report_sales_by_store_location = (
        sales_with_store_details_df
        .groupBy(d_store_df.city.alias("store_city"), d_store_df.country.alias("store_country"))
        .agg(
            F.sum("sale_total_price").alias("location_total_revenue"),
            F.sum("sale_quantity").alias("location_total_quantity_sold")
        )
        .orderBy(F.desc("location_total_revenue"))
    )
    save_report_to_clickhouse(report_sales_by_store_location, "mart_sales_by_store_location",
                              order_by_cols=["store_country", "store_city"])

    # 4.3 –°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –¥–ª—ù –∫–∞–∂–¥–æ–≥–æ –º–∞–≥–∞–∑–∏–Ω–∞
    report_avg_store_order_value = (
        sales_with_store_details_df
        .groupBy(d_store_df.name.alias("store_name"))
        .agg(
            (F.sum("sale_total_price") / F.countDistinct("sale_sk")).alias("avg_transaction_value_per_store")
        )
        .orderBy(F.desc("avg_transaction_value_per_store"))
    )
    save_report_to_clickhouse(report_avg_store_order_value, "mart_avg_store_order_value", order_by_cols=["store_name"])

    print("Generating Supplier Sales Mart...")
    sales_with_supplier_details_df = fact_sales_df.join(d_supplier_df, "supplier_id")

    # 5.1 –¢–æ–ø-5 –ø–æ——Ç–∞–≤—â–∏–∫–æ–≤ — –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤—ã—Ä—É—á–∫–æ–π (–æ—Ç –ø—Ä–æ–¥–∞–∂ —Ç–æ–≤–∞—Ä–æ–≤ ——Ç–∏—Ö –ø–æ——Ç–∞–≤—â–∏–∫–æ–≤)
    report_top_5_suppliers_by_revenue = (
        sales_with_supplier_details_df
        .groupBy(d_supplier_df.name.alias("supplier_name"), d_supplier_df.country.alias("supplier_country"))
        .agg(F.sum("sale_total_price").alias("supplier_generated_revenue"))
        .orderBy(F.desc("supplier_generated_revenue"))
        .limit(5)
    )
    save_report_to_clickhouse(report_top_5_suppliers_by_revenue, "mart_top_5_suppliers_by_revenue",
                              order_by_cols=["supplier_generated_revenue"])

    # 5.2 –°—Ä–µ–¥–Ω—ù—ù —Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–æ–≤ (transaction_unit_price –∏–∑ fact_sales) –æ—Ç –∫–∞–∂–¥–æ–≥–æ –ø–æ—ù—Ç–∞–≤—â–∏–∫–∞
    report_avg_product_price_by_supplier = (
        fact_sales_df
        .join(d_product_df,
              "product_id")
        .join(d_supplier_df, "supplier_id")
        .groupBy(d_supplier_df.name.alias("supplier_name"))
        .agg(F.avg("transaction_unit_price").alias("avg_sold_unit_price_from_supplier"))
        .orderBy(F.desc("avg_sold_unit_price_from_supplier"))
    )
    save_report_to_clickhouse(report_avg_product_price_by_supplier, "mart_avg_product_price_by_supplier",
                              order_by_cols=["supplier_name"])

    # 5.3 –†–∞—ù–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂ –ø–æ —ù—Ç—Ä–∞–Ω–∞–º –ø–æ—ù—Ç–∞–≤—â–∏–∫–æ–≤
    report_sales_by_supplier_country = (
        sales_with_supplier_details_df
        .groupBy(d_supplier_df.country.alias("supplier_country"))
        .agg(
            F.sum("sale_total_price").alias("country_total_revenue"),
            F.sum("sale_quantity").alias("country_total_quantity_sold")
        )
        .orderBy(F.desc("country_total_revenue"))
    )
    save_report_to_clickhouse(report_sales_by_supplier_country, "mart_sales_by_supplier_country",
                              order_by_cols=["supplier_country"])

    # --- –í–∏—Ç—Ä–∏–Ω–∞ –∫–∞—á–µ—ù—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–∏ ---
    print("Generating Product Quality Mart...")
    # 6.1 –ü—Ä–æ–¥—É–∫—Ç—ã —ù –Ω–∞–∏–≤—ã—ù—à–∏–º –∏ –Ω–∞–∏–º–µ–Ω—å—à–∏–º —Ä–µ–π—Ç–∏–Ω–≥–æ–º (–¢–æ–ø-10 –¥–ª—ù –∫–∞–∂–¥–æ–≥–æ)
    report_highest_rated_products = (
        d_product_df.select(
            d_product_df.product_id.alias("business_product_id"),
            d_product_df.name.alias("product_name"),
            d_product_df.rating
        )
        .orderBy(F.desc_nulls_last("rating"))
        .limit(10)
    )
    save_report_to_clickhouse(report_highest_rated_products, "mart_highest_rated_products", order_by_cols=["rating"])

    report_lowest_rated_products = (
        d_product_df.select(
            d_product_df.product_id.alias("business_product_id"),
            d_product_df.name.alias("product_name"),
            d_product_df.rating
        )
        .filter(F.col("rating").isNotNull())
        .orderBy(F.asc_nulls_first("rating"))
        .limit(10)
    )
    save_report_to_clickhouse(report_lowest_rated_products, "mart_lowest_rated_products", order_by_cols=["rating"])

    # 6.2 –ö–æ—Ä—Ä–µ–ª——Ü–∏— –º–µ–∂–¥—É —Ä–µ–π—Ç–∏–Ω–≥–æ–º –∏ –æ–±—ä–µ–º–æ–º –ø—Ä–æ–¥–∞–∂
    # –û–±—ä–µ–º –ø—Ä–æ–¥–∞–∂ - sum(sale_quantity) –¥–ª—ù –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
    product_sales_and_rating_df = (
        fact_sales_df.join(d_product_df, "product_id")
        .groupBy(
            d_product_df.product_id.alias("business_product_id"),
            d_product_df.name.alias("product_name")
        )
        .agg(
            F.avg(d_product_df.rating).alias("avg_product_rating"),
            F.sum(fact_sales_df.sale_quantity).alias("total_product_quantity_sold")
        )
        .filter(F.col("avg_product_rating").isNotNull() & (F.col("total_product_quantity_sold").isNotNull()))
    )

    if product_sales_and_rating_df.count() > 1:
        correlation_value = product_sales_and_rating_df.stat.corr("avg_product_rating", "total_product_quantity_sold")
        print(f"Correlation between product rating and sales volume: {correlation_value}")
    else:
        correlation_value = None  # –∏–ª–∏ 0.0
        print("Not enough data points to calculate correlation between rating and sales volume.")

    correlation_df = spark.createDataFrame(
        [(correlation_value if correlation_value is not None else float('nan'),)],
        ["rating_sales_volume_correlation_coeff"]
    )
    save_report_to_clickhouse(correlation_df, "mart_rating_sales_correlation",
                              order_by_cols=[])

    # 6.3 –ü—Ä–æ–¥—É–∫—Ç—ã —ù –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—ù—Ç–≤–æ–º –æ—Ç–∑—ã–≤–æ–≤ (–¢–æ–ø-10)
    report_top_reviewed_products = (
        d_product_df.select(
            d_product_df.product_id.alias("business_product_id"),
            d_product_df.name.alias("product_name"),
            d_product_df.reviews.alias("number_of_reviews")
        )
        .orderBy(F.desc_nulls_last("number_of_reviews"))
        .limit(10)
    )
    save_report_to_clickhouse(report_top_reviewed_products, "mart_top_reviewed_products",
                              order_by_cols=["number_of_reviews"])

    fact_sales_df.unpersist()
    d_product_df.unpersist()
    d_customer_df.unpersist()
    d_date_df.unpersist()
    d_store_df.unpersist()
    d_supplier_df.unpersist()
    print("All reports generated and saved to ClickHouse. Cached DataFrames unpersisted.")


if __name__ == "__main__":
    spark_session = initialize_spark_session(
        APP_NAME,
        POSTGRES_JDBC_JAR_PATH,
        CLICKHOUSE_JDBC_JAR_PATH
    )
    try:
        generate_reports(spark_session)
    finally:
        print("Stopping Spark session.")
        spark_session.stop()